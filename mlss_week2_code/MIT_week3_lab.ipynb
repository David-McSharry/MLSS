{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import itertools, functools, operator\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEEK 3\n",
    "\n",
    "#theta includes theta0\n",
    "#theta are input as a (n,) array, returns all margins, change output to np.min for overall margin\n",
    "def margin(data, labels, theta):\n",
    "    points_num  = len(labels[0])\n",
    "    ones = np.array([np.ones(points_num)])\n",
    "    data = np.concatenate((data, ones), axis=0)\n",
    "    margins = np.zeros(points_num)\n",
    "    data = data.T\n",
    "    for i in range(points_num):\n",
    "        margins[i] = labels[0][i]*(np.dot(theta,data[i]))/np.linalg.norm(theta[:-1])\n",
    "    return (margins)\n",
    "\n",
    "\n",
    "def cv(data):\n",
    "    return np.array([data])\n",
    "\n",
    "def R(data):\n",
    "    points_num = len(data[0])\n",
    "    ones = np.array([np.ones(points_num)])\n",
    "    data = np.concatenate((data, ones), axis=0)\n",
    "    data = data.T\n",
    "    R_arr = np.zeros(points_num)\n",
    "    for i in range(points_num):\n",
    "        R_arr[i] = np.linalg.norm(data[i])\n",
    "    return np.max(R_arr)\n",
    "\n",
    "def perceptron_max_mistakes(data, labels, theta):\n",
    "    return (R(data)/margin(data, labels, theta))**2\n",
    "\n",
    "#prints 2d data for perceptron\n",
    "def graph_2d(data, labels,theta):\n",
    "    data = data.T\n",
    "    x,y  = data[:,0],data[:,1]\n",
    "    x1,y1 = np.array([]), np.array([])\n",
    "    x2,y2 = np.array([]), np.array([])\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        \n",
    "        if labels[0,i] == 1:\n",
    "            x1 = np.concatenate((x1,np.array([x[i]])))\n",
    "            y1 = np.concatenate((y1,np.array([y[i]])))\n",
    "        else:\n",
    "            x2 = np.concatenate((x2,np.array([x[i]])))\n",
    "            y2 = np.concatenate((y2,np.array([y[i]])))\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(x1, y1, c='b', marker='o')\n",
    "    ax.scatter(x2, y2, c='r', marker='o')\n",
    "    ax.plot()\n",
    "    x = np.linspace(0, 1, 1000)\n",
    "    ax.plot(x , -(theta[0][0]/theta[0][1])*x - (theta[0][2]/theta[0][1]))\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "#this function takes an initial value for theta, for one that defaults to theta_init = 0 see mit_wee2.ipynb\n",
    "def perceptron(data, labels, params = {}, hook = None):    \n",
    "    # if T not in params, default to 100\n",
    "    T = params.get('T', 100)\n",
    "    # Your implementation here\n",
    "    d, n = data.shape\n",
    "    theta = np.zeros((d,1)) \n",
    "    \n",
    "     \n",
    "    misclassified = np.zeros((n,1))\n",
    "    count = 0\n",
    "    theta_0 = np.zeros(1)\n",
    "    #print(\"d = {}, n = {}, theta shape = {}, theta_0 shape = {}\".format(d,n,theta.shape,theta_0.shape))\n",
    "  \n",
    "    for t in range(T):     \n",
    "      for i in range(n):\n",
    "        y = labels[0,i]\n",
    "        x = data[:,i]\n",
    "        \n",
    "        a = np.dot(x,theta)+theta_0\n",
    "        \n",
    "        if np.sign(y*a) <=0: # update the thetas\n",
    "          theta[:,0] = theta[:,0]+ y*x\n",
    "          theta_0 = theta_0 + y\n",
    "          misclassified[i][0] += 1\n",
    "          count += 1\n",
    "        #print(theta, theta_0)\n",
    "          \n",
    "    #print(\"shape x = {}, y = {}, theta = {}, theta_0 = {}\".format(x.shape,y.shape,theta.shape,theta_0.shape))\n",
    "    #print( \"times misclassified = \", misclassified )\n",
    "    print(\"count = \", count)\n",
    "    return (theta,np.array([theta_0]))\n",
    "        \n",
    "def one_hot(x,k):\n",
    "    arr = np.zeros((k,1))\n",
    "    arr[x-1] = 1\n",
    "    return arr\n",
    "          \n",
    "def discrete_to_onehot(data, k):\n",
    "    point_num = len(data[0])\n",
    "    \n",
    "    data_onehot = np.zeros((k,point_num))\n",
    "    for i in range(point_num):\n",
    "        data_onehot[data[0,i]-1,i] = 1\n",
    "    return data_onehot\n",
    "          \n",
    "def signed_dist(x,th,th0):\n",
    "    return (np.dot(th.T,x)+th0)/np.sqrt(np.dot(th.T,th))\n",
    "\n",
    "def poly_features(data, degree):\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    return poly.fit_transform(data)\n",
    "\n",
    "def load_auto_data(path_data):\n",
    "    \"\"\"\n",
    "    Returns a list of dict with keys:\n",
    "    \"\"\"\n",
    "    numeric_fields = {'mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
    "                      'acceleration', 'model_year', 'origin'}\n",
    "    data = []\n",
    "    with open(path_data) as f_data:\n",
    "        for datum in csv.DictReader(f_data, delimiter='\\t'):\n",
    "            for field in list(datum.keys()):\n",
    "                if field in numeric_fields and datum[field]:\n",
    "                    datum[field] = float(datum[field])\n",
    "            data.append(datum)\n",
    "    return data\n",
    "\n",
    "#WEEK 4\n",
    "def f1(x):\n",
    "    return float((2 * x + 3)**2)\n",
    "\n",
    "def df1(x):\n",
    "    return 2 * 2 * (2 * x + 3)\n",
    "\n",
    "\n",
    "def gd(f, df, x0, step_size_fn, max_iter):\n",
    "    x = x0\n",
    "    iter = 0\n",
    "    fs = np.zeros(max_iter, dtype = object)\n",
    "    xs = np.zeros(max_iter, dtype = object)\n",
    "    for i in range(max_iter):\n",
    "        fs[i] = f(x)\n",
    "        xs[i] = x\n",
    "        x  = x - step_size_fn(i) * df(x)\n",
    "        iter += 1\n",
    "    return x, fs, xs\n",
    "\n",
    "def num_grad(f, delta=0.001):\n",
    "    def df(x):\n",
    "        grad = np.zeros_like(x, dtype=object)\n",
    "        delta_i = np.zeros_like(x, dtype=object)\n",
    "        for i in range(len(x)):\n",
    "            delta_i[i] += delta\n",
    "            grad[i] = (f(x+delta_i) - f(x - delta_i)) / (2*delta)\n",
    "            delta_i[i] -= delta\n",
    "        return grad\n",
    "    return df\n",
    "\n",
    "def minimize(f, x0, step_size_fn, max_iter):\n",
    "    df = num_grad(f)\n",
    "    return (gd(f, df, x0, step_size_fn, max_iter))\n",
    "\n",
    "def hinge(v):\n",
    "    arr = [0,1-v]\n",
    "    return max(arr)\n",
    "\n",
    "def hinge_loss(x, y, theta, theta_0):\n",
    "    return hinge(y*(np.dot(theta.T,x) + theta_0))\n",
    "\n",
    "def svm_obj(x, y, theta, theta_0, lam):\n",
    "    n = len(x[0])\n",
    "    L = 0 \n",
    "    for i in range(n):\n",
    "        L += hinge_loss(x[:,i], y[0,i], theta, theta_0)/n\n",
    "    L += lam * np.dot(theta.T,theta)\n",
    "    return L[0]\n",
    "\n",
    "# Returns the gradient of hinge(v) with respect to v.\n",
    "def d_hinge(v):\n",
    "    print(v)\n",
    "    arr = np.zeros_like(v, dtype=object)\n",
    "    for i in range(len(v[0])):\n",
    "        print(v[0][i])\n",
    "        if 1-v[0][i] > 0:\n",
    "            arr[0][i] = -1\n",
    "        else:\n",
    "            arr[0][i] = 0\n",
    "    return arr\n",
    "        \n",
    "def d_hinge_loss_th(x, y, theta, theta_0):\n",
    "    return d_hinge(y*(np.dot(theta.T,x) + theta_0))*y*x\n",
    "\n",
    "def d_hinge_loss_th0(x, y, theta, theta_0):\n",
    "    return d_hinge(y*(np.dot(theta.T,x) + theta_0))*y\n",
    "\n",
    "def d_svm_obj_th(x, y, th, th0, lam):\n",
    "    L_sum = 0\n",
    "    for i in range(len(x[0])):\n",
    "        L_sum += -d_hinge_loss_th(x[:,i], y[0,i], th, th0)*d_hinge(y[0,i]*(np.dot(th.T,x[:,i]) + th0))\n",
    "    L_sum = L_sum/len(x[0])\n",
    "    L_sum += lam * 2 * th.T\n",
    "    return L_sum.T\n",
    "\n",
    "def d_svm_obj_th0(x, y, th, th0, lam):\n",
    "    L_sum = 0\n",
    "    for i in range(len(x[0])):\n",
    "        L_sum += -d_hinge_loss_th0( x[:,i], y[0,i], th, th0 )  *  d_hinge( y[0,i]*(np.dot(th.T,x[:,i]) + th0 ))\n",
    "    L_sum = L_sum/len(x[0])\n",
    "    return L_sum\n",
    "\n",
    "def svm_obj_grad(x,y, th, th0, lam):\n",
    "    return np.vstack([d_svm_obj_th(x,y,th,th0,lam), d_svm_obj_th0(x,y,th,th0,lam)])\n",
    "\n",
    "#broken\n",
    "def batch_svm_min(data, labels, lam):\n",
    "    def f(th):\n",
    "        return svm_obj(data, labels, th[:-1][0], th[-1][0], lam)\n",
    "    def df(th):\n",
    "        return svm_obj_grad(data, labels, th[:-1][0], th[-1][0], lam)\n",
    "    def svm_min_step_size_fn(i):\n",
    "       return 2/(i+1)**0.5\n",
    "    return gd(f, df, np.zeros((3,1)), svm_min_step_size_fn, 100)\n",
    "\n",
    "# WEEK 5\n",
    "\n",
    "# Write a function that returns the gradient of lin_reg(x, th, th0)\n",
    "# with respect to th\n",
    "def d_lin_reg_th(x, th, th0):\n",
    "    return x\n",
    "    \n",
    "# Write a function that returns the gradient of square_loss(x, y, th, th0) with\n",
    "# respect to th.  It should be a one-line expression that uses lin_reg and\n",
    "# d_lin_reg_th.\n",
    "def d_square_loss_th(x, y, th, th0):\n",
    "    return 2*(np.dot(np.transpose(th),x)+th0-y)*d_lin_reg_th(x,th,th0)\n",
    "\n",
    "# Write a function that returns the gradient of mean_square_loss(x, y, th, th0) with\n",
    "# respect to th.  It should be a one-line expression that uses d_square_loss_th.\n",
    "def d_mean_square_loss_th(x, y, th, th0):\n",
    "    return np.array([np.mean(d_square_loss_th(x,y,th,th0), axis=0)]).T\n",
    "\n",
    "\n",
    "def sgd(X, y, J, dJ, w0, step_size_fn, max_iter):\n",
    "    w = w0\n",
    "    fs = []\n",
    "    ws = []\n",
    "    n = len(y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    np.random.seed(0)\n",
    "    for i in range(max_iter):\n",
    "        r = np.random.randint(n)\n",
    "        Xr = X[:,r:r+1]\n",
    "        yr = y[:,r:r+1]\n",
    "        prev_f = J(Xr, yr, w) \n",
    "        prev_grad = dJ(Xr, yr, w)\n",
    "        fs.append(prev_f)\n",
    "        ws.append(w)\n",
    "        if i == max_iter - 1:\n",
    "            return w, fs, ws\n",
    "        step = step_size_fn(i)\n",
    "        w = w - step * prev_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10000000000000009]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1., 2., 3., 4.], [1., 1., 1., 1.]])\n",
    "Y = np.array([[1., 2.2, 2.8, 4.1]])\n",
    "th = np.array([[1.0],[0.05]])\n",
    "th0 = np.array([[0.]])\n",
    "\n",
    "print(d_mean_square_loss_th(X[:,0:1], Y[:,0:1], th, th0).tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ecaf67c2eed83bbe4201b93bdaacc4b80383b39fafae7a462812c913200862b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
